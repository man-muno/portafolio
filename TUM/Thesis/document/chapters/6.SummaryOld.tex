\chapter{Conclusion}
In this last chapter, a discussion about the Rosie where the advantages and disadvantages of the system are described. In addition, some of the work that can be tackled in the future.


%\section{Testing}


\section{Discussion}

In the following section, the proposed system is discussed critically and advantages as well as disadvantages are pointed out. 

It has been said that no battle plan ever survives contact with the enemy, and some disadvantages where recognized when developing Rosie. From the parallel between Some complications were identified while designing the parallel between computer networks and the smart home environment. The first one was with the source material itself. Research on machine learning algorithms for computer networks rely heavily on standardized training sets for they research. That way they can state the performance of the algorithms given a dataset. However, on their findings many suggest that the datasets needed to be cleaned up, and were also processed to extract more features. Unfortunately, the list of new features, or the state of the dataset after it was processed was not usually included with the published findings. This proves to be a problem in the context of this research, because the type of features that can be extracted from the available data are just as important as the selection of the machine learning algorithms themselves.

From the smart home point of view, the prospect is not much better. First, there are no standardized or agreed upon training sets for smart home that are publicly available and can be used for advancing research in the field. The root cause of this problem is that technologies and protocols for computer networks are much more standardized than those for smart environments. Second, the topology of computer networks has also been studied much more than sensor topologies for the smart home environment. This results in an easier job at standardization of datasets for the networking context.

From the architecture point of view, using OpenHAB as the execution environment has a couple of drawbacks. First, 
when setting up a sensor, OpenHAB allows to define information such as types, and also extra information such as groups. However, when implementing a binding all of that extra information is not accessible. When the value for one of the items is read from the event bus, the only information available is the name and the new value for that item. This seems like a missed opportunity to enhance the functionality of the bindings, because that metadata may be useful, specially for automation applications. There are however a couple of workarounds. It is important to note that the metadata is not lost, it is simply not accessible from the event bus. Yet, other interfaces have access to it. It is possible from the binding to query the REST interface to access that metadata, however accessing that metadata through a web interface seems cumbersome and error-prone. \\
The other workaround, the one used by Rosie, is to use the configuration mechanism for the bindings, and write the metadata there as well, to be later stored on the binding. The downside from this approach is that the configuration files have duplicated information. When the amount of sensors is large, maintaining the configuration files is also cumbersome.

From the machine learning point of view, there are some disadvantages from the Rosie system. First, the approach taken by Selim \etAl \cite{hybridMultilevelIntrusion} while developing their hybrid network intrusion detection algorithm is very interesting. They developed a layered architecture for each aspect of the detection process, and then implemented 7 machine learning algorithms and then tested each one of them with the data for each layer. After the testing they did a complete statistical analysis to decide which algorithm was more suitable for each layer of the architecture. This type of approach to selecting algorithms may be beneficial to Rosie. Testing a wider range of algorithms, and statistically analyzing which ones and under what conditions perform better. 
Another disadvantage of the system, how the algorithms learn. First, there is no bulk of data to do off-line training of the algorithms. Second, the algorithms need training data to be able to detect intrusions. Therefore, there will be a time from the moment that the system is started, until the algorithm is trained, that no predictions can be concluded. Using on-line learning algorithms may be a way to tackle that problem. However, the compromise that needs to be reached is being more intrusive when notifying the user about currently sensed events. 

Rosie has however many advantages as well. By implementing the blackboard pattern, the system is able to use different knowledge sources only dependent on the blackboard association, and how the tuples are read from the blackboard. This implies that knowledge sources function as independent, self-contained pieces of code. On one hand, this helps localization, and responsibility assignment, but on the other hand allows to add easily new knowledge sources. \\
It is possible to create new knowledge sources that communicate with external machine learning systems. For example, it is conceivable to write an expert that takes the data stored on the blackboard and translates it into Python commands. That way deep learning libraries written in other languages, that take advantages of GPUs, can be also used. It is also possible to use web-based machine learning frameworks, to off-load the burden of running those algorithms from what may be the underpowered device that runs Rosie.

The classification of the types of sensors (motion, presence, environmental, electrical, and point of entry) allows for the experts to request them from to blackboard according to type. That way, the knowledge sources can easily and directly ask for the information that they require. For example, an expert may only query the information from the presence data and the motion data, to correlate it without needing to explicitly know all the names of all the available motion of presence sensors.\\
Furthermore, this classification allows for sensors to come off-line without affecting the functionality of Rosie, making the system resilient to data loss or sensor failure.\\
Having this classification also allows for more specific data experts to be written. An expert could be written with some set of basic rules that characterize what normal interactions between the sensors are. Not to be used for detection of anomalies, but to be used as classifiers that pre-classify the data to later be fed to the machine learning algorithms. This would reduce the amount of interaction that the user has with the system for on-line machine learning algorithms.\\
Furthermore, the separation of responsibilities from the current architecture, allows without much hassle new experts to be written that extract different set of features from the same data. Enabling exploration of new contexts to find different, or even better, features to be extracted. \\

\section{Future Work}
Both the advantages and disadvantages provide opportunities to build upon the work presented in this thesis, and expand the research on the smart home environments.
From this perspective, one of the identified opportunities is to build better simulations tools. It is difficult for universities to model the behavior of occupants of a home, without building houses and asking researchers to move into them. By having simulations tools, that also connect easily to frameworks like OpenHAB, research will benefit from being able to test multiple algorithms, in multiple models of homes, with different types of occupants and lifestyles, without the need to build multiple houses. Furthermore, with simulation software it is possible to make the passage of time go faster. Training of machine learning algorithms would go rapidly, to later be tested even on real-world 
scenarios. \\
The advantage in developing Rosie is the ease of use when testing automation algorithms on the smart home environment. Using this advantage, developing new machine learning algorithms for the context of smart environments would make development more swift.
From the machine learning perspective, it would be very interesting to develop a new set of features, maybe even based on another discipline, and using Rosie as a test platform for analysis. \\
Also interesting would be connecting Rosie to another web-based system. First, having the power of a data center behind Rosie enables the prospect of using more powerful but also more resource-intensive algorithms. Furthermore, the question of what would happen if Rosie is installed in more than one home is also interesting. If all the houses on a block are equipped with Rosie, they could coordinate the threat level between them, making not only one home safer, but the entire neighborhood.

