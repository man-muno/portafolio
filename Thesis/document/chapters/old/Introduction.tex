\chapter{Introduction}
\label{chapter:Introduction}

\section{Problem Statement}

Recent years have seen the increase of interest in smart homes\cite{googletrends}. The number of products geared towards smart homes has also increased, as it can be seen at trade shows like CES\cite{cnet}. Big manufacturers of consumer electronics have reacted to this trend, and have launched product lines aimed towards that market segment. These products take advantage of wireless communication technologies to control and monitor the environment they are in. The impact of integrating these products into everyday life can enable new ways of interacting with the environments, making them more comfortable, or even more secure.

\subsection*{Problem}
According to the statistics gathered by the German police, 2014 saw the highest number of break-ins in 16 years. In comparison with the previous year, there was a 2\% increase of break-in cases\cite{breakins}. These increasing numbers make the people feel less safe in their own homes.

The best known method to secure a household is to hire a security company to provide, install, and also monitor a security system. When a sensor of the security system is triggered, the security company is notified, and can alert both the authorities and the user. \\
However, this approach has some drawbacks. The installation of the system comes with a price, because it involves breaking the walls to hide cables and panels that are used to control the system. In order to monitor the household, the security company has access to the the security system, which may raise user's privacy concerns. When the system is triggered, the security company reacts by contacting local authorities or the user. Different events such as pets, can trigger the system resulting in a waste of resources.

One way of tackling some of the existing drawbacks of current security systems, is to take advantage of the increasing ubiquity of smart sensors present in smart spaces. By collecting and analyzing data from smart sensors, it is possible to recognize the usual behavior of the inhabitants of the household \cite{behavior}, and separate it from the unusual\cite{anomaly}. 

Registering, recording, and analyzing data from different users, in order to identify unusual and potentially harmful behavior has been used on networking systems for a long time, and some of the concepts and ideas may be find a parallel on the smart home.

\subsection*{Proposed Solution}
Unusual behavior of network users can be recognized using network intrusion detection systems. These systems analyses communication the temporal patterns of network traffic loads and the content of the payload. The same type of information can be found in smart environments. In this thesis, we would like to explore the possibility of applying existing network intrusion detection techniques in smart home environment, with a goal of detecting unusual behavior of inhabitants. Special focus will be set on burglary detection scenarios.

%As a part of this thesis, we foresee the development of a software infrastructure that will enable us to compare the performance of different algorithms used in network intrusion detection systems. The architecture of the software will have to enable simple ways of extending the software with new intrusion detection algorithms. Our software will have to support seamless integration with the existing OpenHab framework.


\section{Related Work}
\label{related_work}

%The Aware Home, Georgia Tech University
\textbf{Kidd \etAl, 1999: The Aware Home: A Living Laboratory for Ubiquitous Computing Research} \cite{raey} \\
This project from the Georgia Institute of Technology, was created as "a living laboratory for interdisciplinary design, development and evaluation"\cite{Kientz:2008:GTA:1358628.1358911}. The objective of the project was to provide a platform where different disciplines could test their hypotheses regarding new technologies for the home environment. The team at the Georgia Institute of Technology built a three-stories house and equipped it with sensors that captured and registered almost every event that happened around the house, using pinhole cameras, microphones, voltmeters, etc. The applications here developed target specific scenarios like supporting aging in place, and supporting busy families. \\
The first scenario addresses problems that stay-at-home senior citizens may have. Issues such as safety, accident prevention and detection, aid in daily activities (reminders and familiarization with new technologies), and facilitating communication with the outside. For example, the project \textbf{Memory Mirror}\cite{Kientz:2008:GTA:1358628.1358911} identifies the objects used by the resident, and post them on a mirror creating a reminder. In the case the user has interacted with the object before, the mirror posts usage statistics.
The supporting busy families scenario is targeted to households where parents work but also need to take care of another family member. The issues include household schedule maintenance, care for individuals with special needs, and make life more enjoyable. The \textbf{Baby Steps} \cite{Kientz:2007:GKU:1240624.1240830} project tracks and logs milestones on the baby's cognitive development cycle, that way if a milestone is missed, the house can provide additional time line information to the doctors.\\
The project from the Georgia Institute of Technology is a good example of what it can be done, when a living space is designed and built as a smart space from the ground up. The house was thought from the start to have a large set of sensors. For example, for activity recognition the Aware Home has 10 pin-hole cameras in just one room\cite{Kientz:2008:GTA:1358628.1358911}. Our project is focused on living spaces that already exist, and it is not possible to install a high number of sensors.\\
From the applications point of view, none of the Aware home project projects tackle the problem of home security or intrusion detection. However, machine learning algorithms are used for activity identification.\\

%MavHome, University of Texas at Arlington
\textbf{Cook \etAl, 2004: MavHome: an agent-based smart home} \cite{1192783} \\
The University of Texas at Arlington proposed an architecture to model the home as an rational agent. It uses sensors to measure what happens on a home, and then uses different actuators to modify it's state. MavHome proposes an architecture that allows the home to learn the behavior of the inhabitants and interactions they have with the home. The agent uses a layered architecture\cite{1192783} with 4 layers. The physical layer communicates the system with the hardware components of the house. The communication layer transmits the data between agents. The information layer is responsible to gather, store, and generate new knowledge that is going to be used on the decision-making process. The decision layer takes the knowledge provided by the information layer and selects actions to be executed. The decision layer has prediction algorithms take sequence of interactions between the inhabitant and the home, and then compares it with previously stored sequences in order to predict the next possible actions. However, the events that can be automated are the ones that occur with certain frequency. \\
To test the project the University also built two physical spaces and a simulation space\cite{inhabitantguidanceofsmartenvironments}. The first space from the physical ones, is a workplace environment equipped with work areas, cubicles, a break room, lounge, and a conference room. The second space is completely equipped apartment. The project also counts with a simulation tool that allows to the machine learning algorithms to be trained, to be later deployed on the physical environments.\\ 
The project arrived at one conclusion that is of importance our hypothesis. Machine learning algorithms can be used to "model and predict inhabitant activities, and that a policy can be learned using this information to automate a smart environment"\cite{inhabitantguidanceofsmartenvironments}.\\

%The Adaptive House, University of Colorado at Boulder
\textbf{Mozer, 2004: Lessons from an adaptive house}\cite{mozer2004lessons}
The Adaptive Home is a project for the University of Colorado at Boulder. It started 1996 with the idea that smart homes should not provide a different control interface, from the one that users normally use. The house uses machine learning algorithms to control heating, ventilation, air conditioning systems, the water heater, and lighting. The Adaptive Home, after observation of the interaction between the house and the users, can deduct patterns and make predictions. If the users are not happy with the predictions, they can change the values selected by the house using the normal interface. The main objective of the Adaptive Home is comfort while reducing operating costs\cite{mozer2004lessons}.\\
The algorithm uses two constraints that need to be optimally satisfied: cost and discomfort. Through reinforcement learning, it is possible to calculate the next action for the current state, or for the next predicted state. Coarse activity identification is also one of the main focuses on the project. Once an activity is identified, the system can trigger an event that in turn will change the state of the house\cite{mozer2004lessons} into the predicted one.\\
This project not only boosts the idea from using machine learning on the home environment, but also provides an interesting perspective to that aspect, because it includes the discomfort constraint. The system calculates the cost of performing an action, not only with monetary value but also with how annoying the action can be if it is wrong. It is also interesting that it does not discretized time into timed intervals, but into event changes that trigger states changes.\\
For this project security of the smart home was not explored. However, the use of machine learning algorithms support the idea that it is possible to do activity identification, and execute actions according to those states. \\

%Aire project: Focuses on work spaces

%CyberManor, Internet Home Alliance. Company, not a Uni project
%CASAS Smart home project. Data sets MavHome comes from them.
%EasyLiving, Microsoft
%Hal, MIT Artificial Intelligence Laboratory. No home automation project
%Home Automation, IBM

%House_n, MIT
\textbf{Intille, 2004: The Goal: Smart People, Not Smart Homes} \cite{smartpeople} \\
The Massachusetts Institute of Technology also decided to explore the smart home with a multi-disciplinary approach. Many industry partners and other departments at MIT co-founded the House\underline{ }n project. The goal of this project \cite{smartpeople} was to shift the investigation's focus from a smart home that automates all possible activities, to one where the smart home requires user's interaction to help maintaining life stimulating.\cite{smartpeople}.\\
Within the House\underline{ }n project, MIT joined forces with industry partners used the approach of building a live-in laboratory. There, volunteers would move in for a period of time and lead their lives normally, while also being measured by many sensors. The laboratory was built in 2004 and it is called the PlaceLab\cite{placelab}. 
The focus of this project\cite{placelab} was proactive health care. By providing relevant, just-in-time information\cite{dietary} the home influence the occupants to make better dietary choices. Also, the project focuses on the influence of technology can impact independence of aging seniors\cite{aginginplace}, using different methods of indoor positioning\cite{bhack}, and the implications of user tracking\cite{info:doi/10.2196/jmir.8.4.e29}. All other projects tackled problems from other disciplines like material engineering and architecture.\\
%IIB, Trinity College Dublin
%i-LAND, Ambiente
%Interactive Workspaces, Stanford University. collaborative work settings
%Neural network house from 1995. May be too old.
%PRIMA, Inria
%The intelligent home 1999. May be too old.
%Smart Spaces Lab, NIST. Project for working spaces detecting meetigns 
%SMART Connected Home, GE

%Machine learning 
\textbf{Fung \etAl, 2013: Intrusion Detection Networks: A Key to Distributed Security} \cite{fung2013CRC} \\
Intrusion detection systems are designed to monitor all the elements deployed on a network, and alert in case out of the ordinary behavior is detected. Network intrusion detection techniques can be divided into two: Signature-based and anomaly-based\cite{fung2013CRC}. Signature-based techniques use a predefined list of already identified intrusion methods, and compares the network's traffic to filter out possible intrusion attempts. Anomaly-based techniques compare the behavior of the network with a precollected baseline of normal behavior. This technique is the one we want to explore with our current work, under what circumstances anomaly detection can be extended to the smart environment, and which of the intrusion detection methods can be also used on the home.\\
 
\textbf{Bhattacharyya \etAl, 2013: Network Anomaly Detection: A Machine Learning Perspective} \cite{Bhattacharyya:2013:NAD:2505468} \\
The work of Bhattacharyya \etAl analyzes thoroughly the different machine learning techniques used on anomaly-based network intrusion detection. According to the authors, it is possible to analyze the traffic on a network, and then classify it between malicious or normal behavior. This characteristic turns the anomaly detection problem into a classification problem\cite{Bhattacharyya:2013:NAD:2505468}.\\
The authors also present a list of requirements that must be met by real-time or near real-time systems, that can be adapted to the smart home environment: 
1) These systems must detect anomalies with high detection accuracy with minimal or even no prior knowledge of the environment, and also must observe that environment and learn what is the normal behavior of it. 
2) The identification process must be done with the minimum amount of false positives. 
3) The number of features should be the least possible. The authors also propose other two requirements, which they are implicit to the smart environment.

The work of Bhattacharyya \etAl also categorizes the machine learning techniques used for network anomaly detection into six categories. 
	\begin{itemize}
		\item \textbf{Supervised learning}
		
This method assumes that it is possible to start with a batch of labeled data called a training set. This collection of data is used to build a model, to which all new data can be compared against and decide if the behavior is normal or not. This categorization is divided into parametric and nonparametric methods. \\
The first approach assumes that the data is produced using a parametric distribution and probability density function.
The second approach, the nonparametric method, does not start with a model but creates one from the data. The most used nonparametric method for anomaly detection is to take the training set and create a function that maps the date to the interval from [0,1]. Then, a data point is evaluated with that function, and if it scores under a certain predefined threshold then is considered anomalous.\\
One of the advantages of using supervised learning is that if the system runs for long enough, it is possible to establish a very good baseline to be used as a comparison. The disadvantages include susceptibility to being trained to consider as malicious traffic as normal. Also, due to the assumptions made by the model and on the grounds that setting all the parameters is very difficult, the ratio between false positives and false negatives is unfavorable. The disadvantages seem to outweigh the advantages.\\

		\item \textbf{Unsupervised learning}
		
For supervised learning it is very important to have labeled data. However obtaining that data may be very difficult or expensive, since the use of experts is needed. Methods that do not rely on labels are called unsupervised, and they may be better suited to set the baseline that represents normal behavior. \\

One of the methods is \textit{clustering}. Here a representative point is selected for each desired cluster, then each test data point is taken, and classified according to the nearest cluster. Here the disadvantage of an expert becomes again apparent. However works like UNADA\cite{unada} where Casas \etAl{} introduced a clustering algorithm for unsupervised network anomaly knowledge-independent detection of anomalous traffic. This algorithm uses a clustering technique to identify clusters and outliers in multiple low-dimensional spaces.\\

%needs explanation of how it works and how it can be translated to smart homes
\textit{Association mining} tries to identify events that happen together. This technique assumes that patterns in the data can be identified. Typical usage of association mining, is when stores perform market research to better stock their shelfs, analyzing the data it is possible to come interesting conclusions. For example, customers from a convenience store that buy product A always end up buying also product B on the current or future visits. With that sort of information the market can stock product A and B together. That is way some stores set aside shelf space for lemons on the alcoholic beverages aisle. Intrusion detection algorithms such as ADAM\cite{Barbara:2001:ATE:604264.604268} use this principles to identify out-of-the-ordinary behavior. \\

\textit{Outlier mining} takes the opposite approach of clustering. Here the techniques search for the data points that do not represent the majority of the data. This points are called outliers. These methods are distance-based, density-based
and soft computing approaches. The first one defines a function that measures the distance of the new data point in comparison to the training set. The selection of this function depends on the assumptions made on the data\cite{Gogoi:2011:SOD:1971494.1971504}. Density-based methods estimate the density distribution
of the data instance and then identify outliers as
those who are located in regions of low density\cite{Koufakou:2010:FOD:1743225.1743230}.\\

		\item \textbf{Probabilistic learning}
		
The techniques grouped by probabilistic learning allow for including random behavior or with probabilistic uncertainty. According to  Bhattacharyya \etAl the main characteristic of these techniques is they allow to "update previous outcome estimates by conditioning them with newly available evidence"\cite{Bhattacharyya:2013:NAD:2505468}. This seems like a desirable attribute to have, when evaluating human behavior, where probabilistic uncertainty us inherent.\\
%Hidden Markov Model based methods. 

\textit{Bayesian networks} is a model where the relationships between random variables are represented. Usually a direct graph is used in order to represent the random variables and the dependency relation between them. On the nodes of the graph also are incorporated the possible states of the random variable, and the conditional probability table\cite{Kruegel:2003:BEC:956415.956436}. The relation that the arcs exhibit is one of causality, that means that the child node is causally dependent of the parent nodes.\cite{Kruegel:2003:BEC:956415.956436}. \\
Kruegel \etAl explain in \cite{Kruegel:2003:BEC:956415.956436} that given the assumptions made with Bayesian networks, it is very likely to have a high number of false alarms, when used on network intrusion detection methods. Given the low probability of the parent variable, e.g. "an intrusion is occurring", and the dependence of the child's variable probability on the parents probability., e.g. "a motion sensor is tripped given that an intrusion is occurring".  \\

\textit{Hidden Markov models} is a mechanism to assign probability distributions to a sequence of observations at equally-spaced time intervals. Even thought this is a type of Bayesian network, they differ due on some of the extra assumptions that are made\cite{Ghahramani:2001:IHM:505741.505743}. The process is called \textit{hidden} because the observations are caused by states which are hidden to the observer. The other assumption is that the Markov property is fulfilled, that means, the current state is independent from all the past states. Also, implies that the current state contains all the information of the past states needed to calculate any future states. Bhattacharyya \etAl estate that it is possible to train the models by using data obtained by doing packet inspection, as well as system calls and commands.\\

\textit{Naive Bayes} based techniques have been also used on this field. Naive Bayes methods are a simplified version of the Bayesian network model\cite{Langley:1992:ABC:1867135.1867170}. Here, independence between the variables is assumed. That way, it is possible to calculate the a posteriori probability of an outcome given the a priori observations. Bhattacharyya \etAl and Kruegel \etAl note that there are however some limitations to the use of this technique. First, the assumed independence between the variables gives it the same classification capability as other simpler techniques, like threshold-base systems. Second, adding new information problematic. \\

\textit{Gaussian mixture model} based techniques are also used. This approach also assumes independence between the variables, however it uses Gaussian probability distribution functions to build the probability distribution each individual variable. This functions are called Gaussian mixtures. Bahrololum \etAl \cite{Bahrololum2008} developed an anomaly detection system using Gaussian mixture models. There, they use maximum likelyhood estimation to estimate the parameters of the Gaussian mixture model. Once the models were set up, they where trained using a famous dataset, which included instances of different network attacks, such as probing, denial-of-service, unauthorized access to local super user (root) privileges, and unauthorized access from a remote machine\cite{Bahrololum2008}. \\

%EM Model		

		\item \textbf{Soft computing}
		
		
Soft computing can be view as the opposite of traditional computing that requires a precise, even ideal, established analytical mode. Soft computing much like the human mind is tolerant of imprecision and uncertainty. This techniques often do not find exact solutions, and that makes them suitable for network anomaly detection.\\

The \textit{Genetic algorithms} approach represent a problem on a data structure much like chromosomes. Then, operators, like selection, recombination, and mutation are applied in order to generate new sample points in a search space\cite{geneticAlgos}. 
In the context of network anomaly detection, the chromosomes represent attributes like services, flags, number of super user attempts\cite{Bhattacharyya:2013:NAD:2505468}. \\	
Balajinath \etAl propose a network anomaly intrusion detection system based on genetic algorithms. This algorithm continuously captures and learns the user behavior in order to identify when that behavior is abnormal.  \cite{Balajinath:2001:IDT:2294491.2294970} Each command of the user forms a gene, and together with a fitness function calculated for a set of genes, new iteration of genes are computed to determine the probability of commands being intrusive.\\

\textit{Artificial neural networks} can also be used for anomaly and network intrusion detection. Neural networks are motivated on how fundamentally different the human brain and a computer process and compute information. The hierarchical multi-layered structure is composed of interconnected processing elements working in conjunction to solve specific problems. ANNs are commonly used on various applications such as data clustering, feature extraction and anomalous pattern identification.\\
The work by Lee \etAl\cite{935046} present a very interesting approach to network intrusion detection. They use a hierarchy of back propagation neural networks to detect in real time known and unknown attacks. Each neural network instead of being built with network data, they are built with the normal behavior contained on the TCP protocol.\\

\textit{Rough Sets} introduced by Zdzislaw \cite{Pawlak199748} deals with sets that include vagueness in its definition. The opposite are traditional sets also known as crisp sets. On a crisp set all the mathematical notions must be exact. On the other hand, rough sets define a boundary region, a lower approximation, and an upper approximation. The lower approximation consists of all objects which surely belong to the set. The upper approximation contains all objects which possibly belong to the set. Objects that cannot be classified with certainty as members of the set completely. The difference between the upper and the lower approximation constitute the boundary region of the vague concept.
Rough Sets  methods have been also researched as techniques for intrusion detection. Bhattacharyya \etAl explain that these techniques are useful on network intrusion detection because of their simplicity, and because they allow learning with small training datasets\cite{Bhattacharyya:2013:NAD:2505468}. Rough Sets have been used on network intrusion detection systems in conjunction with another techniques like fuzzy set theory \cite{4509827} and support vector machines\cite{5176039}.\\


Boolean logic is the one used by computers where the state of all elements can only be true or false. In \textit{fuzzy logic} that set is opened to more possible values. For example, when talking about the weather one can say that today the weather is great, good, ok, bad, or awful. An algorithm was develop by Dikerson \etAl to use fuzzy logic and statistics to identify network anomalies\cite{877441}. After a learning phase, ranges for the types of data being monitored are evaluated. Five fuzzy sets LOW, MEDIUM-LOW, MEDIUM, MEDIUM-HIGH, and HIGH, where determined to apply the same fuzzy rules to all the network input. Tajbakhsh \etAl also applied this concept to network intrusion detection. They extracted a set of fuzzy association rules for different classes, and is used as a model for each class. To determine the class of a set of transactions, they generate a set of fuzzy association rules from this transaction set and compute the similarity of the extracted rule set with sets mined from each class\cite{Tajbakhsh2009462}. The fuzzy association rule sets are used to describe normal and anomalous classes. A sample is considered normal if the compatibility of the rule set generated is above a certain threshold. Those with lower compatibility are regarded as anomalous.

A fuzzy class association rule mining method based on \textit{genetic network programming} was presented by Mabu \etAl. GNP is similar to genetic algorithms, but instead of strings to denote genes it uses directed graphs, which boosts the representation capability and node's reusability in a graph structure. Using both fuzzy set theory with GNP, this method can use discrete and continuous values to extract important class-association rules that boost detection ability\cite{5499108}.

%Xian et al. [388] propose a novel unsupervised fuzzy clustering method based on clonal selection for anomaly detection. The method is capable of obtaining global optimal clusters more quickly than competing algorithms.

\textit{Human Immune systems} have been also used to model algorithms called artificial immune systems. The objective of the human immune system is to recognize anomalies within the body. The immune system classifies some external objects that enter the body as undesirable and creates antibodies to fight those antigens. The process of identifying the antigens is called negative selection. This process has been used as motivation to develop network intrusion detection algorithms \cite{ais}, however the outcome is that the time need to create all the detectors to achieve high confidence is impractical for high dimensional data.


%		\item Knowledge-based  
%		
%		
%Rule based and Expert System based
%
%Ontology and Logic System based

		
%		\item Combination learners
%		
%		
%Ensamble based
%
%Fusion based
%
%Hybrid
		
	\end{itemize}


\section{Terminology}

The following section introduces definitions of ambiguous words that are used continuously throughout this thesis. 

\begin{itemize}

\item \textbf{Data point} 

\item \textbf{Training set} 

\item \textbf{smart environment} 


\end{itemize}
